{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#To frame the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#to scrape the data\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#to read the data as html, text\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#to characterize the text data\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.reddit.com\"## YOUR CODE HERE\n",
    "response = requests.get(URL, headers = {'User-agent': 'MrMagic'})\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def list_subreddits(iterations= 2, user_output= 'A'):\n",
    "    \n",
    "    #re-instantiate iterations\n",
    "    itera = iterations\n",
    "    \n",
    "    user_iterations = input('Iterations/pages (default = 2): ')\n",
    "    if user_iterations != '':\n",
    "        itera = int(user_iterations)\n",
    "        \n",
    "    \n",
    "    #defaults to the full dataframe, any other argument gives a test output\n",
    "    user_output_choice = user_output\n",
    "    choice = input('\"\\n\"Type A for the full dataframe (default).\\\n",
    "                   \"\\n\"Type B for the data withouth central tendencies.\\\n",
    "                   \"\\n\"Type C for the central tendencies.\\\n",
    "                   \"\\n\"Type D for the subbreddit linksoup.')\n",
    "\n",
    "    if choice != '':\n",
    "        user_output_choice = choice\n",
    "    \n",
    "    \n",
    "    #go to URL, turn the page to text, get html \n",
    "    URL = \"https://www.reddit.com\"## YOUR CODE HERE\n",
    "    response = requests.get(URL, headers = {'User-agent': 'MrMagic'})\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    #create an empty dictionary\n",
    "    dicti = {}\n",
    "    \n",
    "    #create empty lists for the title, time, and subreddit category, and comment\n",
    "    list_title = []\n",
    "    list_time = []\n",
    "    list_subs = []\n",
    "    list_comments = []\n",
    "    \n",
    "    \n",
    "    #comments will have to be stripped of their string values, leaving the comment counts\n",
    "    list_comment_counts = []\n",
    "    \n",
    "    \n",
    "    #dump the link soup here\n",
    "    list_linksoup = []\n",
    "    \n",
    "    #comment rate is the comment count/hours\n",
    "    list_cRate = []\n",
    "    \n",
    "    #gather links\n",
    "    list_p = []\n",
    "    list_a = []\n",
    "\n",
    "    while itera > 0:\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            #identify the tags containing the features of interest\n",
    "            title_tags = soup.find_all('a', {'data-event-action' : \"title\"})\n",
    "            time_tags = soup.find_all('time',  {'class':'live-timestamp'})\n",
    "            sub_tags = soup.find_all('a', {'class':'subreddit hover may-blank'})\n",
    "            sub_comments = soup.find_all('a', {'data-event-action':'comments'})\n",
    "            link = soup.find_all('span', {'class': 'next-button'})\n",
    "            p_set = soup.find_all('p', {'class': 'title'})\n",
    "            \n",
    "            #append various feature lists\n",
    "            for tag in title_tags:\n",
    "                list_title.append(tag.text)\n",
    "            for tag in time_tags:\n",
    "                list_time.append(tag.text)\n",
    "            for tag in sub_tags:\n",
    "                list_subs.append(tag.text)\n",
    "            for tag in sub_comments:\n",
    "                list_comments.append(tag.text)\n",
    "            for i in range(len(p_set)):\n",
    "                list_p.append(soup.find_all('p', {'class': 'title'})[i])\n",
    "                list_a.append(list_p[i]('a')[0]['href'])\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "            #the next button link URL based on the comment count of 25 per page, assigned to variable 'link'\n",
    "            link_tag = soup.find_all('span', {'class', 'next-button'})\n",
    "            link = link_tag[0]('a')[0]['href']\n",
    "            response = requests.get(link, headers = {'User-agent': 'MrMagic'})\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            #add a sleep command\n",
    "            time.sleep(1) \n",
    "\n",
    "            itera -= 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "   \n",
    "    for i in range(len(list_a)):\n",
    "        if list_a[i][:2] == '/r':\n",
    "            list_a[i] = 'https://www.reddit.com' + list_a[i]    \n",
    "    for i in list_comments:\n",
    "        list_comment_counts.append(i[:-9])\n",
    "    for lst in list_time:\n",
    "        list_cRate.append(lst)\n",
    "    for i in range(len(list_cRate)):\n",
    "        list_cRate[i] = int(list_comment_counts[i])/float(list_cRate[i][:-9].strip()) #ugly float\n",
    "    for i in range(len(list_a)):\n",
    "        url = list_a[i]\n",
    "        response = requests.get(URL, headers = {'User-agent': 'MrGumbo'})\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        list_linksoup.append(soup)\n",
    "    \n",
    "    \n",
    "    #cleaning up the comment rate output\n",
    "    list_cRate = [ '%.1f' % i for i in list_cRate]                  \n",
    "    list_cRate = [float(i) for i in list_cRate] \n",
    "    \n",
    "    \n",
    "   ################################################################## \n",
    "\n",
    "     # comment count central tendancies\n",
    "    list_comment_counts = [float(i) for i in list_comment_counts]\n",
    "    \n",
    "    list_comment_median_threshold = []\n",
    "    for i in range(len(list_comment_counts)):\n",
    "        if list_comment_counts[i] >= np.median(list_comment_counts):\n",
    "            list_comment_median_threshold.append(1)\n",
    "        else:\n",
    "            list_comment_median_threshold.append(0)\n",
    "    \n",
    "    list_comment_mean_threshold = []\n",
    "    for i in range(len(list_comment_counts)):\n",
    "        if list_comment_counts[i] >= np.mean(list_comment_counts):\n",
    "            list_comment_mean_threshold.append(1)\n",
    "        else:\n",
    "            list_comment_mean_threshold.append(0)\n",
    "    \n",
    "    # comment rate central tendancies\n",
    "    list_cRate_mean_threshold = []\n",
    "    for i in range(len(list_cRate)):\n",
    "        if list_cRate[i] >= np.mean(list_cRate):\n",
    "            list_cRate_mean_threshold.append(1)\n",
    "        else:\n",
    "            list_cRate_mean_threshold.append(0)\n",
    "            \n",
    "    list_cRate_median_threshold = []\n",
    "    for i in range(len(list_cRate)):\n",
    "        if list_cRate[i] >= np.median(list_cRate):\n",
    "            list_cRate_median_threshold.append(1)\n",
    "        else:\n",
    "            list_cRate_median_threshold.append(0)     \n",
    "            \n",
    "    ##################################################################  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #building the dictionary\n",
    "    dicti['Above_comment_mean'] = list_comment_mean_threshold\n",
    "    dicti['Above_comment_median'] = list_comment_median_threshold\n",
    "    dicti['Above_cRate_mean'] = list_cRate_mean_threshold\n",
    "    dicti['Above_cRate_median'] = list_comment_median_threshold\n",
    "    dicti['subreddit_linksoup'] = list_linksoup\n",
    "    dicti['title'] = list_title\n",
    "    dicti['time'] = list_time\n",
    "    dicti['subreddit'] = list_subs\n",
    "    dicti['comment_counts'] = list_comment_counts\n",
    "    dicti['sub_link'] = list_a\n",
    "    dicti['comment_per_hour'] = list_cRate\n",
    "    \n",
    "    #convert the dictionary to a dataframe, sorted by the comment rate in descending order\n",
    "    pg_df = pd.DataFrame(dicti)\n",
    "    pg_df = pg_df.sort_values(['comment_per_hour'], ascending = False)  #sort by comment rate\n",
    "                               \n",
    "    thresh = pd.DataFrame({'comment_mean': list_comment_median_threshold,\n",
    "                           'comment_median': list_comment_median_threshold,\n",
    "                           'cRate_mean': list_cRate_mean_threshold,\n",
    "                           'cRate_median': list_cRate_median_threshold})\n",
    "    #return pg_df[['title', 'subreddit', 'sub_link', 'comment_counts','comment_per_hour', 'time', ]\n",
    "    \n",
    "\n",
    "    df1 = pg_df[['title', 'subreddit', 'sub_link', 'comment_counts','comment_per_hour', 'time', ]]\n",
    "    df2 = pg_df[['title', 'Above_comment_mean', 'Above_comment_median', 'Above_cRate_mean', 'Above_cRate_median']]\n",
    "    df3 = pg_df[['title', 'subreddit_linksoup']]\n",
    "    \n",
    "    if user_output_choice.lower() == 'a':\n",
    "        return pg_df\n",
    "    elif user_output_choice.lower() == 'b':\n",
    "        return df1\n",
    "    elif user_output_choice.lower() == 'c':\n",
    "        return df2\n",
    "    elif user_output_choice.lower() == 'd':\n",
    "        return df3\n",
    "    else:\n",
    "        None\n",
    "            \n",
    "                               \n",
    "                               \n",
    "                               \n",
    "                               \n",
    "                               \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations/pages (default = 2): 2\n",
      "\"\n",
      "\"Type A for the full dataframe (default).                   \"\n",
      "\"Type B for the data withouth central tendencies.                   \"\n",
      "\"Type C for the central tendencies.                   \"\n",
      "\"Type D for the subbreddit linksoup.\n"
     ]
    }
   ],
   "source": [
    "# 1 second sleep timer\n",
    "Reddit_analyzer = list_subreddits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Above_cRate_mean</th>\n",
       "      <th>Above_cRate_median</th>\n",
       "      <th>Above_comment_mean</th>\n",
       "      <th>Above_comment_median</th>\n",
       "      <th>comment_counts</th>\n",
       "      <th>comment_per_hour</th>\n",
       "      <th>sub_link</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_linksoup</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2972.0</td>\n",
       "      <td>495.3</td>\n",
       "      <td>https://i.imgur.com/mThH44E.gifv</td>\n",
       "      <td>r/gaming</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\n",
       "&lt;html lang=\"en\" xml:lang=\"en\" ...</td>\n",
       "      <td>6 hours ago</td>\n",
       "      <td>This is how i see my retirement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>431.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>https://www.reddit.com/r/pics/comments/80dbt6/...</td>\n",
       "      <td>r/nottheonion</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\n",
       "&lt;html lang=\"en\" xml:lang=\"en\" ...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>President Trump: I would have run into school ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Above_cRate_mean  Above_cRate_median  Above_comment_mean  \\\n",
       "12                 1                   1                   1   \n",
       "32                 1                   1                   0   \n",
       "\n",
       "    Above_comment_median  comment_counts  comment_per_hour  \\\n",
       "12                     1          2972.0             495.3   \n",
       "32                     1           431.0             431.0   \n",
       "\n",
       "                                             sub_link      subreddit  \\\n",
       "12                   https://i.imgur.com/mThH44E.gifv       r/gaming   \n",
       "32  https://www.reddit.com/r/pics/comments/80dbt6/...  r/nottheonion   \n",
       "\n",
       "                                   subreddit_linksoup         time  \\\n",
       "12  <!DOCTYPE html>\n",
       "<html lang=\"en\" xml:lang=\"en\" ...  6 hours ago   \n",
       "32  <!DOCTYPE html>\n",
       "<html lang=\"en\" xml:lang=\"en\" ...   1 hour ago   \n",
       "\n",
       "                                                title  \n",
       "12                    This is how i see my retirement  \n",
       "32  President Trump: I would have run into school ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reddit_analyzer.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reddit_analyzer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv(Reddit_analyzer['text_content'], stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Greg code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Count vectorize the title field\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_vals = cv.fit_transform(Reddit_analyzer['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# create X df from title (count vectorized) and subreddit (get dummied)\n",
    "X = pd.concat([pd.get_dummies(Reddit_analyzer['subreddit']).set_index(np.arange(pd.get_dummies(Reddit_analyzer['subreddit']).shape[0]), inplace=True)\n",
    "           , pd.DataFrame(cv_vals.todense(), columns=cv.get_feature_names())]\n",
    "          , ignore_index=False\n",
    "          , axis=1\n",
    ")\n",
    "# create y from CRate and comment median fields (this is a 50x4 y target, not a single class)\n",
    "y = Reddit_analyzer.iloc[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi output classifier allows us to do a multi-column y target with a normal classifier like logistic regression\n",
    "moc = MultiOutputClassifier(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "           n_jobs=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46666666666666667"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing accuracy of the model\n",
    "moc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Above_cRate_mean',\n",
       " 'Above_cRate_median',\n",
       " 'Above_comment_mean',\n",
       " 'Above_comment_median']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the column headers of the predicted y target\n",
    "y_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.58852083  0.41147917] [ 0.39543286  0.60456714] [ 0.52939052  0.47060948] [ 0.39543286  0.60456714]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.74471064  0.25528936] [ 0.46102994  0.53897006] [ 0.61341479  0.38658521] [ 0.46102994  0.53897006]\n",
      "[ 0.77016677  0.22983323] [ 0.50736314  0.49263686] [ 0.62215191  0.37784809] [ 0.50736314  0.49263686]\n",
      "[ 0.7578281  0.2421719] [ 0.5744063  0.4255937] [ 0.70644788  0.29355212] [ 0.5744063  0.4255937]\n",
      "[ 0.6428277  0.3571723] [ 0.43686801  0.56313199] [ 0.58142081  0.41857919] [ 0.43686801  0.56313199]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n",
      "[ 0.72292577  0.27707423] [ 0.50189839  0.49810161] [ 0.66034715  0.33965285] [ 0.50189839  0.49810161]\n"
     ]
    }
   ],
   "source": [
    "# probability class labels of 'Above_cRate_mean', 'Above_cRate_median', 'Above_comment_mean', 'Above_comment_median'\n",
    "# i.e. [ 0.72292577  0.27707423] is a 72% probability of Above_cRate_mean being 1 and 27% probability of being 0\n",
    "for i, j, k, l in zip(moc.predict_proba(X_test)[0], \n",
    "                moc.predict_proba(X_test)[1],\n",
    "                moc.predict_proba(X_test)[2],\n",
    "                moc.predict_proba(X_test)[3]):\n",
    "    print(i,j, k, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVERYTHING BELOW THIS CELL IS NOTES ONLY"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 second sleep\n",
    "\n",
    "start = time.time()\n",
    "a = list_subreddits(10)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    ">23.246593952178955"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 second sleep\n",
    "\n",
    "start = time.time()\n",
    "a = list_subreddits(12)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    ">27.108170986175537"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 second sleep\n",
    "\n",
    "start = time.time()\n",
    "a = list_subreddits(15)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-32-959dcba0d14d> in <module>()\n",
    "      6 '''\n",
    "      7 start = time.time()\n",
    "----> 8 a = list_subreddits(15)\n",
    "      9 end = time.time()\n",
    "     10 print(end - start)\n",
    "\n",
    "<ipython-input-21-ba6600144a49> in list_subreddits(iterations)\n",
    "     78         list_cRate.append(lst)\n",
    "     79     for i in range(len(list_cRate)):\n",
    "---> 80         list_cRate[i] = float(list_comment_counts[i])/int(list_cRate[i][:-9]) #ugly float\n",
    "     81 \n",
    "     82     list_cRate = [ '%.1f' % i for i in list_cRate]                  #tidy float\n",
    "\n",
    "ValueError: invalid literal for int() with base 10: '2 mi'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " #Was experimenting with ways to package each page iteration into a separate dataframe, for ultimate compilation \n",
    " \n",
    " #if iterations > 10\n",
    "    lst_dfs= []\n",
    "    if iterations > 10:\n",
    "        iterad = iteration / 10\n",
    "        iterar = iteration % 10\n",
    "        n_dataframes = iterad + 1\n",
    "        for i in range(n_dataframes):\n",
    "            lst_dfs.append('df_'+ str(int(i+1)))\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stay above this code for practice/scratch... the goal is to iterate beyone 20 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def list_subreddits(iterations= 1):\n",
    "    \n",
    "    #go to URL, turn the page to text, get html \n",
    "    URL = \"https://www.reddit.com\"## YOUR CODE HERE\n",
    "    response = requests.get(URL, headers = {'User-agent': 'MrMagic'})\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    dicti = {}\n",
    "    list_title = []\n",
    "    list_time = []\n",
    "    list_subs = []\n",
    "    list_comments = []\n",
    "    list_comment_counts = []\n",
    "    list_p = []\n",
    "    list_a = []sleep_val = iteration - 4\n",
    "    list_linksoup = []\n",
    "    list_cRate = []\n",
    "    \n",
    "    itera = iterations\n",
    "    \n",
    "    if iterations > 19:\n",
    "        iterad = iteration / 19\n",
    "        iterar = iteration % 19\n",
    "        \n",
    "    n_dataframes = [iterad + 1 if iterar > 0 else iterad]\n",
    "    lst_dfs = []\n",
    "    for i in range(n_dataframes):\n",
    "        lst_dfs.append('df_'+ str(i))\n",
    "    \n",
    "    while itera > 0:\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            #identify the tags containing the features of interest\n",
    "            title_tags = soup.find_all('a', {'data-event-action' : \"title\"})\n",
    "            time_tags = soup.find_all('time',  {'class':'live-timestamp'})\n",
    "            sub_tags = soup.find_all('a', {'class':'subreddit hover may-blank'})\n",
    "            sub_comments = soup.find_all('a', {'data-event-action':'comments'})\n",
    "            link = soup.find_all('span', {'class': 'next-button'})\n",
    "            p_set = soup.find_all('p', {'class': 'title'})\n",
    "\n",
    "            #append various feature lists\n",
    "            for tag in title_tags:\n",
    "                list_title.append(tag.text)\n",
    "            for tag in time_tags:\n",
    "                list_time.append(tag.text)\n",
    "            for tag in sub_tags:\n",
    "                list_subs.append(tag.text)\n",
    "            for tag in sub_comments:\n",
    "                list_comments.append(tag.text)\n",
    "            for i in range(len(p_set)):\n",
    "                list_p.append(soup.find_all('p', {'class': 'title'})[i])\n",
    "                list_a.append(list_p[i]('a')[0]['href'])\n",
    "\n",
    "\n",
    "            #the next button link URL based on the comment count of 25 per page, assigned to variable 'link'\n",
    "            link_tag = soup.find_all('span', {'class', 'next-button'})\n",
    "            link = link_tag[0]('a')[0]['href']\n",
    "            response = requests.get(link, headers = {'User-agent': 'MrMagic'})\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            #add a sleep command\n",
    "            time.sleep(1) #defined as a function of number of iterations\n",
    "\n",
    "            itera -= 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for i in range(len(list_a)):\n",
    "        if list_a[i][:2] == '/r':\n",
    "            list_a[i] = 'https://www.reddit.com' + list_a[i]    \n",
    "    for i in list_comments:\n",
    "        list_comment_counts.append(i[:-9])\n",
    "    for lst in list_time:\n",
    "        list_cRate.append(lst)\n",
    "    for i in range(len(list_cRate)):\n",
    "        list_cRate[i] = float(list_comment_counts[i])/int(list_cRate[i][:-9]) #ugly float\n",
    "    \n",
    "    list_cRate = [ '%.1f' % i for i in list_cRate]                  #tidy float\n",
    "    list_cRate = [float(i) for i in list_cRate]                     #had to be redundant for some reason?\n",
    "    dicti['title'] = list_title\n",
    "    dicti['time'] = list_time\n",
    "    dicti['subreddit'] = list_subs\n",
    "    dicti['comment_counts'] = list_comment_counts\n",
    "    dicti['sub_link'] = list_a\n",
    "    dicti['comment_per_hour'] = list_cRate\n",
    "    \n",
    "    #convert the dictionary to a dataframe, sorted by the comment rate in descending order\n",
    "    pg_df = pd.DataFrame(dicti)\n",
    "    pg_df = pg_df.sort_values(['comment_per_hour'], ascending = False)  #sort by comment rate\n",
    "    #pg_df = pg_df[pg_df.time != '1 hour ago']\n",
    "    \n",
    "    \n",
    "    #return pg_df[['title', 'subreddit', 'sub_link', 'comment_counts', 'time']]\n",
    "    return pg_df[['title', 'subreddit', 'sub_link', 'comment_counts','comment_per_hour', 'time', ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(list_subreddits(5))   \n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nineteen_iterations = list_subreddits(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_iterations = list_subreddits(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ten_iterations)\n",
    "ten_iterations\n",
    "#next, try sort_values by time then comment_count,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "working pairs of time in seconds values versus maximum iterations\n",
    " * longer sleep time facilitates higher iterations\n",
    "\n",
    "[time.sleep(i): max_iterations] =\n",
    "    3:7\n",
    "    4:8\n",
    "    5:9 > 225\n",
    "    5:10 > PVE\n",
    "    6:10 > PVE\n",
    "    5:9 > error\n",
    "    5:9 > 225\n",
    "    6:10 > PVE\n",
    "    7:10 > error\n",
    "    1:4 > 100\n",
    "\n",
    "Try/Except has allowed for\n",
    "    1:10 > 250"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PREVAILING ERROR MESSAGE\n",
    "\n",
    "\n",
    "<ipython-input-581-876da8d34118> in list_subreddits(iterations)\n",
    "     67             list_a[i] = 'https://www.reddit.com' + list_a[i]\n",
    "     68     for i in list_comments:\n",
    "---> 69         list_comment_counts.append(int(i[:-9]))\n",
    "     70     for lst in list_time:\n",
    "     71         list_cRate.append(lst)\n",
    "\n",
    "ValueError: invalid literal for int() with base 10: ''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5:9 pair error??\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-584-a23def53158a> in <module>()\n",
    "----> 1 length = len(list_subreddits(9))\n",
    "\n",
    "<ipython-input-583-68fb87db6a1f> in list_subreddits(iterations)\n",
    "     71         list_cRate.append(lst)\n",
    "     72     for i in range(len(list_cRate)):\n",
    "---> 73         list_cRate[i] = float(list_comment_counts[i])/int(list_cRate[i][:-9]) #ugly float\n",
    "     74 \n",
    "     75     list_cRate = [ '%.1f' % i for i in list_cRate]                  #tidy float\n",
    "\n",
    "ValueError: invalid literal for int() with base 10: '13 mi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subreddits()['comment_per_hour'] = [float(i) for i in list_subreddits()['comment_per_hour']]\n",
    "tst\n",
    "tst = [float(i) for i in tst]\n",
    "tst\n",
    "tst.sort()\n",
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lengths = [len(list_subreddits(i)) for i in range(8,12,2)].... does not work\n",
    "    #time.sleep(3)\n",
    "    \n",
    "#time.sleep(3)    \n",
    "    #length = [len(list_subreddits(i)) for i in range(4,8,2)]... does work > [100, 150]\n",
    "    #length = len(list_subreddits(7))... does work > 175\n",
    "    #length = len(list_subreddits(8))... does not work > ValueError: invalid literal for int() with base 10: ''\n",
    "\n",
    "#time.sleep(4)\n",
    "    #length = len(list_subreddits(7))... does work > 175\n",
    "    #length = len(list_subreddits(8))... does work > 200\n",
    "    #length = len(list_subreddits(10))... does not work\n",
    "    #length = len(list_subreddits(9)) ... does not work \n",
    "    #length = len(list_subreddits(8))... does work > 200\n",
    "    \n",
    "#time.sleep(5)\n",
    "    #length = len(list_subreddits(8))... does work > 200\n",
    "    #length = len(list_subreddits(9))... does work > 225\n",
    "    #length = len(list_subreddits(10))... does not work\n",
    "\n",
    "#time.sleep(6)\n",
    "    #length = len(list_subreddits(8))... does work\n",
    "    #length = len(list_subreddits(10))...\n",
    "\n",
    "#wo\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure out how to list subreddit links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
