{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Web Scraping for Reddit & Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data by scraping a website and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to the overall interaction (as measured by number of comments)?_\n",
    "\n",
    "Your method for acquiring the data will be scraping the 'hot' threads as listed on the [Reddit homepage](https://www.reddit.com/). You'll acquire _AT LEAST FOUR_ pieces of information about each thread:\n",
    "1. The title of the thread\n",
    "2. The subreddit that the thread corresponds to\n",
    "3. The length of time it has been up on Reddit\n",
    "4. The number of comments on the thread\n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts whether or not a given Reddit post will have above or below the _median_ number of comments.\n",
    "\n",
    "**BONUS PROBLEMS**\n",
    "1. If creating a logistic regression, GridSearch Ridge and Lasso for this model and report the best hyperparameter values.\n",
    "1. Scrape the actual text of the threads using Selenium (you'll learn about this in Webscraping II).\n",
    "2. Write the actual article that you're pitching and turn it into a blog post that you host on your personal website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. Use BeautifulSoup to parse the page and extract all results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.datastories.com/blog/reddit-front-page\n",
    "\n",
    "Top page Reddit posts stay on the front page an average of 4 hours 15 minutes.\n",
    "The average lifetime on the front page of an image is 3.5 hours, while the text posts live for 4 hours and 45 minutes on average.\n",
    "Internal self-posts LIVE SIGNIFICANTLY LONGER than external posts.\n",
    "The average lifetime of a Reddit's self post is 5 hours and 15 minutes.\n",
    "The average lifetime of an external post is only 3 hours and 45 minutes.\n",
    "The average lifetime of text posts with a positive headline is significantly longer than the lifetime of posts with a neutral or negative headline.\n",
    "Textual self-posts with positive headlines stay significantly longer on the front page.\n",
    "Starting at 9am PST is the fastest time for getting upvotes.\n",
    "For text posts, Very Positive or Very Negative posts perform significantly better than Neutral ones.\n",
    "Images get much more upvotes than text posts.\n",
    "However….text posts get more comments and stay on the front page longer.\n",
    "There are 5 Sub-Reddits that completely dominate the front page of Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Plan\n",
    "\n",
    "# scrape multiple times, eg top 100 posts, 3*day for 3 days, or more often\n",
    "# score/ upvotes, downvotes, # comments, rank, time posted, len time on reddit\n",
    "# title: sentiment?, numbers?, special char?\n",
    "# content for text posts, sentiment, subreddit, # subscribers per sub, image?, video?, podcast?,\n",
    "#     internal/external, \n",
    "\n",
    "## other - how long was it in the top 100? , what freq can i scrape....\n",
    "# analyze title - is there a clickbait formula?\n",
    "# time it takes to get to front page (top25)\n",
    "# does the OP engage in the comments section? how much?\n",
    "# has OP gone viral before? how often?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this has some more verbose elements removed, we can see that there is some structure to the above:\n",
    "- The thread title is within an `<a>` tag with the attribute `data-event-action=\"title\"`.\n",
    "- The time since the thread was created is within a `<time>` tag with attribute `class=\"live-timestamp\"`.\n",
    "- The subreddit is within an `<a>` tag with the attribute `class=\"subreddit hover may-blank\"`.\n",
    "- The number of comments is within an `<a>` tag with the attribute data-event-action=\"comments\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sleepy scraper\n",
    "# def scraper(url):\n",
    "#     time.sleep(2)\n",
    "#     response = requests.get(url)\n",
    "#     print('status code', response.status_code)\n",
    "#     html = response.text\n",
    "#     return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.reddit.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = requests.get(url, headers= {'User-Agent': 'kittenMitten'})\n",
    "# html_doc = html.text\n",
    "# soup = BeautifulSoup(html_doc, 'lxml')\n",
    "# print(soup.prettify())\n",
    "html = requests.get(url, headers= {'User-Agent': 'kittenMittenz'})\n",
    "html_doc = html.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "# time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(html.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write 4 functions to extract these items (one function for each): title, time, subreddit, and number of comments.¶\n",
    "Example\n",
    "```python\n",
    "def extract_title_from_result(result):\n",
    "    return result.find ...\n",
    "```\n",
    "\n",
    "##### - Make sure these functions are robust and can handle cases where the data/field may not be available.\n",
    ">- Remember to check if a field is empty or None for attempting to call methods on it\n",
    ">- Remember to use try/except if you anticipate errors.\n",
    "\n",
    "- **Test** the functions on the results above and simple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Notice that within the div tag there is \n",
    "#an attribute called id and it is set to \"thing_t3_788tye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### title EDA\n",
    "# has emoji\n",
    "# has numbers\n",
    "# sentiment\n",
    "# len(title)\n",
    "# title structure... ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles(url, mysoup):\n",
    "    return [titles.text for titles in\n",
    "                  mysoup.find_all('a', {\"data-event-action\": \"title\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subreddit(url, mysoup):\n",
    "    return [subreddit.text.replace('r/', '') for subreddit in\n",
    "                     mysoup.find_all('a', {\"class\": \"subreddit hover may-blank\"})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeup(url, mysoup):\n",
    "    return [timeup.text.replace(' hours ago', '') for timeup in\n",
    "                  mysoup.find_all('time', {\"class\": \"live-timestamp\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_comments(url, mysoup):\n",
    "    return [num_comments.text.replace(' comments', '') for num_comments in\n",
    "                        mysoup.find_all('a', {\"data-event-action\": \"comments\"})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": true,
    "id": "7203e0c9-e437-4802-a6ad-7dc464f94436"
   },
   "source": [
    "Now, to scale up our scraping, we need to accumulate more results.\n",
    "\n",
    "First, look at the source of a Reddit.com page: (https://www.reddit.com/).\n",
    "Try manually changing the page by clicking the 'next' button on the bottom. Look at how the url changes.\n",
    "\n",
    "After leaving the Reddit homepage, the URLs should look something like this:\n",
    "```\n",
    "https://www.reddit.com/?count=25&after=t3_787ptc\n",
    "```\n",
    "\n",
    "The URL here has two query parameters\n",
    "- count is the result number that the page starts with\n",
    "- after is the unique id of the last result on the _previous_ page\n",
    "\n",
    "In order to scrape lots of pages from Reddit, we'll have to change these parameters every time we make a new request so that we're not just scraping the same page over and over again. Incrementing the count by 25 every time will be easy, but the bizarre code after `after` is a bit trickier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, let's look at a block of HTML from a Reddit page to see how we might solve this problem:\n",
    "```html\n",
    "<div class=\" thing id-t3_788tye odd gilded link \" data-author=\"LordSneaux\" data-author-fullname=\"t2_j3pty\" data-comments-count=\"1548\" data-context=\"listing\" data-domain=\"v.redd.it\" data-fullname=\"t3_788tye\" data-kind=\"video\" data-num-crossposts=\"0\" data-permalink=\"/r/funny/comments/788tye/not_all_heroes_wear_capes/\" data-rank=\"25\" data-score=\"51468\" data-subreddit=\"funny\" data-subreddit-fullname=\"t5_2qh33\" data-timestamp=\"1508775581000\" data-type=\"link\" data-url=\"https://v.redd.it/ush0rh2tultz\" data-whitelist-status=\"all_ads\" id=\"thing_t3_788tye\" onclick=\"click_thing(this)\">\n",
    "      <p class=\"parent\">\n",
    "      </p>\n",
    "      <span class=\"rank\">\n",
    "       25\n",
    "      </span>\n",
    "      <div class=\"midcol unvoted\">\n",
    "       <div aria-label=\"upvote\" class=\"arrow up login-required access-required\" data-event-action=\"upvote\" role=\"button\" tabindex=\"0\">\n",
    "       </div>\n",
    "       <div class=\"score dislikes\" title=\"53288\">\n",
    "        53.3k\n",
    "       </div>\n",
    "       <div class=\"score unvoted\" title=\"53289\">\n",
    "        53.3k\n",
    "       </div>\n",
    "       <div class=\"score likes\" title=\"53290\">\n",
    "        53.3k\n",
    "       </div>\n",
    "       <div aria-label=\"downvote\" class=\"arrow down login-required access-required\" data-event-action=\"downvote\" role=\"button\" tabindex=\"0\">\n",
    "       </div>\n",
    "      </div>\n",
    "```\n",
    "\n",
    "Notice that within the `div` tag there is an attribute called `id` and it is set to `\"thing_t3_788tye\"`. By finding the last ID on your scraped page, you can tell your _next_ request where to start (pass everything after \"thing_\").\n",
    "\n",
    "For more info on this, you can take a look at the [Reddit API docs](https://github.com/reddit/reddit/wiki/JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write one more function that finds the last `id` on the page, and stores it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_reddit_id(url, mysoup): \n",
    "    return mysoup.find('div', attrs={'class': 'nav-buttons'}).find('span', attrs={'class': 'next-button'}).find('a').attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Collect more information\n",
    "\n",
    "While we only require you to collect four features, there may be other info that you can find on the results page that might be useful. Feel free to write more functions so that you have more interesting and useful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likes(url, mysoup):\n",
    "    return [likes.text for likes in\n",
    "                 mysoup.find_all('div', {'class', 'score likes'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's put it all together.\n",
    "\n",
    "Use the functions you wrote above to parse out the 4 fields - title, time, subreddit, and number of comments. Create a dataframe from the results with those 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url):\n",
    "    html = requests.get(url, headers= {'User-Agent': 'puppies'})\n",
    "    print(html.status_code)\n",
    "    html_doc = html.text\n",
    "    mysoup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        titles_list = titles(url, mysoup)\n",
    "        timeup_list = timeup(url, mysoup)\n",
    "        subreddit_list = subreddit(url, mysoup)\n",
    "        num_comments_list = num_comments(url, mysoup)\n",
    "        likes_list = likes(url, mysoup)\n",
    "\n",
    "#         get last reddit id on page for next page url\n",
    "        lrid = last_reddit_id(url, mysoup)\n",
    "\n",
    "        reddit = pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame({'title': titles_list, 'timeup [hours_ago]': timeup_list, \n",
    "                   'subreddit': subreddit_list, 'num_comments': num_comments_list, \n",
    "                       'likes': likes_list})\n",
    "\n",
    "        return df, lrid\n",
    "    \n",
    "    except:\n",
    "        print('NOOOOOOOOOOOOOOOOOOOOOOOO!!!!!!!!!!!')\n",
    "\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop num:\t 0 url is:\t http://www.reddit.com/\n",
      "200\n",
      "loop num:\t 25 url is:\t https://www.reddit.com/?count=25&after=t3_80wjzd\n",
      "200\n",
      "loop num:\t 50 url is:\t https://www.reddit.com/?count=50&after=t3_80vx0y\n",
      "200\n",
      "loop num:\t 75 url is:\t https://www.reddit.com/?count=75&after=t3_80uukq\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < 100:\n",
    "\n",
    "    if i < 1:\n",
    "        url = \"http://www.reddit.com/\"\n",
    "        print('loop num:\\t', i, 'url is:\\t', url)\n",
    "        reddit, lrid = scraper(url)\n",
    "        i += 25\n",
    "\n",
    "    else:\n",
    "        url = lrid\n",
    "        print('loop num:\\t', i, 'url is:\\t', url)\n",
    "        new_df, new_url = scraper(url)\n",
    "        reddit = reddit.append(new_df, ignore_index=True)\n",
    "        lrid = new_url\n",
    "        i += 25\n",
    "\n",
    "#     reddit['rank'] = reddit.index.get_values() + 1\n",
    "#     reddit['datetime'] = datetime = html.headers['Date']\n",
    "    \n",
    "# df.to_csv('/Users/meredithjackson/Desktop/ga/week5/reddit_data.csv')\n",
    "# print(\"status code: \" + str(html.status_code))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timeup [hours_ago]</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>•</td>\n",
       "      <td>359</td>\n",
       "      <td>sports</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>\"Just stay in there, you're done for tonight\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.2k</td>\n",
       "      <td>523</td>\n",
       "      <td>pics</td>\n",
       "      <td>4</td>\n",
       "      <td>Butterflies will sometimes land on a Caiman an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.9k</td>\n",
       "      <td>822</td>\n",
       "      <td>funny</td>\n",
       "      <td>4</td>\n",
       "      <td>Two drunk gentlemen try to pass each other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9808</td>\n",
       "      <td>112</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>4</td>\n",
       "      <td>Hey guys, I know I'm late to the meme party bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.7k</td>\n",
       "      <td>413</td>\n",
       "      <td>gifs</td>\n",
       "      <td>4</td>\n",
       "      <td>These VR apps are getting out of hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.5k</td>\n",
       "      <td>211</td>\n",
       "      <td>NatureIsFuckingLit</td>\n",
       "      <td>3</td>\n",
       "      <td>Octopus riding sea turtle 🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29.3k</td>\n",
       "      <td>526</td>\n",
       "      <td>OldSchoolCool</td>\n",
       "      <td>5</td>\n",
       "      <td>Bride leaving her recently bombed home to get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6802</td>\n",
       "      <td>60</td>\n",
       "      <td>StrangerThings</td>\n",
       "      <td>3</td>\n",
       "      <td>Feeling cute, might delete later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9694</td>\n",
       "      <td>219</td>\n",
       "      <td>gifsthatkeepongiving</td>\n",
       "      <td>3</td>\n",
       "      <td>Two drunk gentlemen try to pass each other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1k</td>\n",
       "      <td>296</td>\n",
       "      <td>DIY</td>\n",
       "      <td>4</td>\n",
       "      <td>Perpetual Flip Calendar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7795</td>\n",
       "      <td>251</td>\n",
       "      <td>HistoryPorn</td>\n",
       "      <td>4</td>\n",
       "      <td>President Truman, wearing a shirt that reads '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.9k</td>\n",
       "      <td>859</td>\n",
       "      <td>gaming</td>\n",
       "      <td>4</td>\n",
       "      <td>Fallout in a nutshell.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.1k</td>\n",
       "      <td>274</td>\n",
       "      <td>Unexpected</td>\n",
       "      <td>4</td>\n",
       "      <td>Attracting the right people!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.9k</td>\n",
       "      <td>453</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>6</td>\n",
       "      <td>\"You reach in there you not gon live to reach ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22.8k</td>\n",
       "      <td>364</td>\n",
       "      <td>aww</td>\n",
       "      <td>5</td>\n",
       "      <td>Born to wear socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6918</td>\n",
       "      <td>608</td>\n",
       "      <td>TwoXChromosomes</td>\n",
       "      <td>3</td>\n",
       "      <td>Pence believes abortion will end \"in our time,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15.8k</td>\n",
       "      <td>176</td>\n",
       "      <td>PeopleFuckingDying</td>\n",
       "      <td>4</td>\n",
       "      <td>crueL oWNER BoILs hIS HElPlesS LoyaL dOggO ALi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.8k</td>\n",
       "      <td>612</td>\n",
       "      <td>ATBGE</td>\n",
       "      <td>5</td>\n",
       "      <td>No, it's not too much chrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6251</td>\n",
       "      <td>395</td>\n",
       "      <td>pics</td>\n",
       "      <td>3</td>\n",
       "      <td>The Arctic 100 years ago and today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9144</td>\n",
       "      <td>275</td>\n",
       "      <td>streetwear</td>\n",
       "      <td>4</td>\n",
       "      <td>[ART] Vibrant Street Look.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.4k</td>\n",
       "      <td>132</td>\n",
       "      <td>reallifedoodles</td>\n",
       "      <td>5</td>\n",
       "      <td>Catcha da fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.8k</td>\n",
       "      <td>225</td>\n",
       "      <td>AnimalsBeingDerps</td>\n",
       "      <td>5</td>\n",
       "      <td>This is as good as any place to sit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5342</td>\n",
       "      <td>217</td>\n",
       "      <td>whitepeoplegifs</td>\n",
       "      <td>4</td>\n",
       "      <td>Guy nails a half court shot to win 12 medium p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22.5k</td>\n",
       "      <td>274</td>\n",
       "      <td>CrappyDesign</td>\n",
       "      <td>5</td>\n",
       "      <td>Ｔｕｒｎ ｍｅ ｏｎ ｙｏｕｒｓｅｌｆ．</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5742</td>\n",
       "      <td>88</td>\n",
       "      <td>SequelMemes</td>\n",
       "      <td>4</td>\n",
       "      <td>Parental supervision advised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.3k</td>\n",
       "      <td>379</td>\n",
       "      <td>CringeAnarchy</td>\n",
       "      <td>6</td>\n",
       "      <td>Classic non-political cringe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.4k</td>\n",
       "      <td>720</td>\n",
       "      <td>hmmm</td>\n",
       "      <td>6</td>\n",
       "      <td>hmmm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7105</td>\n",
       "      <td>79</td>\n",
       "      <td>drunk</td>\n",
       "      <td>6</td>\n",
       "      <td>This speaks volumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.0k</td>\n",
       "      <td>649</td>\n",
       "      <td>gifs</td>\n",
       "      <td>6</td>\n",
       "      <td>Do the split</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20.4k</td>\n",
       "      <td>1316</td>\n",
       "      <td>technology</td>\n",
       "      <td>7</td>\n",
       "      <td>Washington just passed the country’s toughest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>79.6k</td>\n",
       "      <td>2598</td>\n",
       "      <td>funny</td>\n",
       "      <td>9</td>\n",
       "      <td>Ugly people have talent too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>11.1k</td>\n",
       "      <td>123</td>\n",
       "      <td>teenagers</td>\n",
       "      <td>9</td>\n",
       "      <td>Same when your parents ask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>50.2k</td>\n",
       "      <td>10730</td>\n",
       "      <td>news</td>\n",
       "      <td>6</td>\n",
       "      <td>Dick’s, Major Gun Retailer, Will Stop Selling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>10.2k</td>\n",
       "      <td>149</td>\n",
       "      <td>softwaregore</td>\n",
       "      <td>7</td>\n",
       "      <td>Cute dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>10.5k</td>\n",
       "      <td>390</td>\n",
       "      <td>iamverysmart</td>\n",
       "      <td>8</td>\n",
       "      <td>High IQ family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>3159</td>\n",
       "      <td>72</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>4</td>\n",
       "      <td>A character in a video game talking about how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>16.0k</td>\n",
       "      <td>173</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>7</td>\n",
       "      <td>With great reflexes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2837</td>\n",
       "      <td>98</td>\n",
       "      <td>StarWars</td>\n",
       "      <td>5</td>\n",
       "      <td>Cant believe I got to experience this, serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4760</td>\n",
       "      <td>109</td>\n",
       "      <td>pics</td>\n",
       "      <td>6</td>\n",
       "      <td>The evolution of dudes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4371</td>\n",
       "      <td>157</td>\n",
       "      <td>4PanelCringe</td>\n",
       "      <td>6</td>\n",
       "      <td>The baddest in the universe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1622</td>\n",
       "      <td>23</td>\n",
       "      <td>DadReflexes</td>\n",
       "      <td>3</td>\n",
       "      <td>Right before a date..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4571</td>\n",
       "      <td>33</td>\n",
       "      <td>aww</td>\n",
       "      <td>6</td>\n",
       "      <td>Australian Sea Lion mumma and pup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>3934</td>\n",
       "      <td>41</td>\n",
       "      <td>BikiniBottomTwitter</td>\n",
       "      <td>7</td>\n",
       "      <td>College.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4611</td>\n",
       "      <td>39</td>\n",
       "      <td>disneyvacation</td>\n",
       "      <td>6</td>\n",
       "      <td>How to Engage in Competitive Grieving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5044</td>\n",
       "      <td>104</td>\n",
       "      <td>surrealmemes</td>\n",
       "      <td>7</td>\n",
       "      <td>$̸̮̲̪͔̯̻̦̙̒͛͂̌͛̍̃̚0̵̼̰͕̳̘͈̻͖̊̀̚ͅ0̵͒̏̋̈́͑̐̈́͛̃̋̌͠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>45.8k</td>\n",
       "      <td>284</td>\n",
       "      <td>wholesomememes</td>\n",
       "      <td>10</td>\n",
       "      <td>T-Pain being super wholesome about Marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2130</td>\n",
       "      <td>45</td>\n",
       "      <td>Justrolledintotheshop</td>\n",
       "      <td>4</td>\n",
       "      <td>I think our shop toilet may be stolen....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2264</td>\n",
       "      <td>80</td>\n",
       "      <td>harrypotter</td>\n",
       "      <td>4</td>\n",
       "      <td>I made some necklace pendants :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2297</td>\n",
       "      <td>29</td>\n",
       "      <td>EarthPorn</td>\n",
       "      <td>2</td>\n",
       "      <td>Lightroom Ice Cave, Vatnajökull Glacier, Vatna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2008</td>\n",
       "      <td>97</td>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>4</td>\n",
       "      <td>ULPT: Potential employer asking about gaps in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1144</td>\n",
       "      <td>450</td>\n",
       "      <td>news</td>\n",
       "      <td>2</td>\n",
       "      <td>NFL names Pizza Hut as its new official pizza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1647</td>\n",
       "      <td>72</td>\n",
       "      <td>gaming</td>\n",
       "      <td>2</td>\n",
       "      <td>A Minecraft Voxelart piece I recently finished...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1933</td>\n",
       "      <td>22</td>\n",
       "      <td>greentext</td>\n",
       "      <td>3</td>\n",
       "      <td>Anon teaches a lesson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>6642</td>\n",
       "      <td>1280</td>\n",
       "      <td>PoliticalHumor</td>\n",
       "      <td>8</td>\n",
       "      <td>Voting is Not a Winter Solstice Holiday Card o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4154</td>\n",
       "      <td>38</td>\n",
       "      <td>firstworldanarchists</td>\n",
       "      <td>7</td>\n",
       "      <td>Samsung with an absolute classic on the S9 pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>7482</td>\n",
       "      <td>124</td>\n",
       "      <td>BigCatGifs</td>\n",
       "      <td>9</td>\n",
       "      <td>Big cat but still just a kitty at heart.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3456</td>\n",
       "      <td>112</td>\n",
       "      <td>Tinder</td>\n",
       "      <td>5</td>\n",
       "      <td>I think I finally met my soul mate on Tinder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3588</td>\n",
       "      <td>208</td>\n",
       "      <td>mildlyinfuriating</td>\n",
       "      <td>6</td>\n",
       "      <td>The curse of the left handed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2590</td>\n",
       "      <td>91</td>\n",
       "      <td>Naruto</td>\n",
       "      <td>6</td>\n",
       "      <td>Done a drawing of Sasuke. What do u think guys?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>23.9k</td>\n",
       "      <td>890</td>\n",
       "      <td>educationalgifs</td>\n",
       "      <td>9</td>\n",
       "      <td>Now all picture is clear for me that Who can d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    likes num_comments              subreddit timeup [hours_ago]  \\\n",
       "0       •          359                 sports         1 hour ago   \n",
       "1   45.2k          523                   pics                  4   \n",
       "2   33.9k          822                  funny                  4   \n",
       "3    9808          112          DunderMifflin                  4   \n",
       "4   49.7k          413                   gifs                  4   \n",
       "5   18.5k          211     NatureIsFuckingLit                  3   \n",
       "6   29.3k          526          OldSchoolCool                  5   \n",
       "7    6802           60         StrangerThings                  3   \n",
       "8    9694          219   gifsthatkeepongiving                  3   \n",
       "9   10.1k          296                    DIY                  4   \n",
       "10   7795          251            HistoryPorn                  4   \n",
       "11  22.9k          859                 gaming                  4   \n",
       "12  10.1k          274             Unexpected                  4   \n",
       "13  18.9k          453     BlackPeopleTwitter                  6   \n",
       "14  22.8k          364                    aww                  5   \n",
       "15   6918          608        TwoXChromosomes                  3   \n",
       "16  15.8k          176     PeopleFuckingDying                  4   \n",
       "17  14.8k          612                  ATBGE                  5   \n",
       "18   6251          395                   pics                  3   \n",
       "19   9144          275             streetwear                  4   \n",
       "20  14.4k          132        reallifedoodles                  5   \n",
       "21  19.8k          225      AnimalsBeingDerps                  5   \n",
       "22   5342          217        whitepeoplegifs                  4   \n",
       "23  22.5k          274           CrappyDesign                  5   \n",
       "24   5742           88            SequelMemes                  4   \n",
       "25  12.3k          379          CringeAnarchy                  6   \n",
       "26  26.4k          720                   hmmm                  6   \n",
       "27   7105           79                  drunk                  6   \n",
       "28  13.0k          649                   gifs                  6   \n",
       "29  20.4k         1316             technology                  7   \n",
       "..    ...          ...                    ...                ...   \n",
       "70  79.6k         2598                  funny                  9   \n",
       "71  11.1k          123              teenagers                  9   \n",
       "72  50.2k        10730                   news                  6   \n",
       "73  10.2k          149           softwaregore                  7   \n",
       "74  10.5k          390           iamverysmart                  8   \n",
       "75   3159           72         Showerthoughts                  4   \n",
       "76  16.0k          173                  Jokes                  7   \n",
       "77   2837           98               StarWars                  5   \n",
       "78   4760          109                   pics                  6   \n",
       "79   4371          157           4PanelCringe                  6   \n",
       "80   1622           23            DadReflexes                  3   \n",
       "81   4571           33                    aww                  6   \n",
       "82   3934           41    BikiniBottomTwitter                  7   \n",
       "83   4611           39         disneyvacation                  6   \n",
       "84   5044          104           surrealmemes                  7   \n",
       "85  45.8k          284         wholesomememes                 10   \n",
       "86   2130           45  Justrolledintotheshop                  4   \n",
       "87   2264           80            harrypotter                  4   \n",
       "88   2297           29              EarthPorn                  2   \n",
       "89   2008           97   UnethicalLifeProTips                  4   \n",
       "90   1144          450                   news                  2   \n",
       "91   1647           72                 gaming                  2   \n",
       "92   1933           22              greentext                  3   \n",
       "93   6642         1280         PoliticalHumor                  8   \n",
       "94   4154           38   firstworldanarchists                  7   \n",
       "95   7482          124             BigCatGifs                  9   \n",
       "96   3456          112                 Tinder                  5   \n",
       "97   3588          208      mildlyinfuriating                  6   \n",
       "98   2590           91                 Naruto                  6   \n",
       "99  23.9k          890        educationalgifs                  9   \n",
       "\n",
       "                                                title  \n",
       "0       \"Just stay in there, you're done for tonight\"  \n",
       "1   Butterflies will sometimes land on a Caiman an...  \n",
       "2          Two drunk gentlemen try to pass each other  \n",
       "3   Hey guys, I know I'm late to the meme party bu...  \n",
       "4               These VR apps are getting out of hand  \n",
       "5                         Octopus riding sea turtle 🔥  \n",
       "6   Bride leaving her recently bombed home to get ...  \n",
       "7                    Feeling cute, might delete later  \n",
       "8          Two drunk gentlemen try to pass each other  \n",
       "9                             Perpetual Flip Calendar  \n",
       "10  President Truman, wearing a shirt that reads '...  \n",
       "11                             Fallout in a nutshell.  \n",
       "12                       Attracting the right people!  \n",
       "13  \"You reach in there you not gon live to reach ...  \n",
       "14                                 Born to wear socks  \n",
       "15  Pence believes abortion will end \"in our time,...  \n",
       "16  crueL oWNER BoILs hIS HElPlesS LoyaL dOggO ALi...  \n",
       "17                       No, it's not too much chrome  \n",
       "18                 The Arctic 100 years ago and today  \n",
       "19                         [ART] Vibrant Street Look.  \n",
       "20                                     Catcha da fish  \n",
       "21                This is as good as any place to sit  \n",
       "22  Guy nails a half court shot to win 12 medium p...  \n",
       "23                               Ｔｕｒｎ ｍｅ ｏｎ ｙｏｕｒｓｅｌｆ．  \n",
       "24                       Parental supervision advised  \n",
       "25                      Classic non-political cringe.  \n",
       "26                                               hmmm  \n",
       "27                                This speaks volumes  \n",
       "28                                       Do the split  \n",
       "29  Washington just passed the country’s toughest ...  \n",
       "..                                                ...  \n",
       "70                        Ugly people have talent too  \n",
       "71                         Same when your parents ask  \n",
       "72  Dick’s, Major Gun Retailer, Will Stop Selling ...  \n",
       "73                                           Cute dog  \n",
       "74                                     High IQ family  \n",
       "75  A character in a video game talking about how ...  \n",
       "76                             With great reflexes...  \n",
       "77  Cant believe I got to experience this, serious...  \n",
       "78                            The evolution of dudes.  \n",
       "79                        The baddest in the universe  \n",
       "80                              Right before a date..  \n",
       "81                  Australian Sea Lion mumma and pup  \n",
       "82                                           College.  \n",
       "83              How to Engage in Competitive Grieving  \n",
       "84  $̸̮̲̪͔̯̻̦̙̒͛͂̌͛̍̃̚0̵̼̰͕̳̘͈̻͖̊̀̚ͅ0̵͒̏̋̈́͑̐̈́͛̃̋̌͠...  \n",
       "85        T-Pain being super wholesome about Marriage  \n",
       "86          I think our shop toilet may be stolen....  \n",
       "87                   I made some necklace pendants :)  \n",
       "88  Lightroom Ice Cave, Vatnajökull Glacier, Vatna...  \n",
       "89  ULPT: Potential employer asking about gaps in ...  \n",
       "90  NFL names Pizza Hut as its new official pizza ...  \n",
       "91  A Minecraft Voxelart piece I recently finished...  \n",
       "92                              Anon teaches a lesson  \n",
       "93  Voting is Not a Winter Solstice Holiday Card o...  \n",
       "94  Samsung with an absolute classic on the S9 pro...  \n",
       "95           Big cat but still just a kitty at heart.  \n",
       "96       I think I finally met my soul mate on Tinder  \n",
       "97                    The curse of the left handed...  \n",
       "98    Done a drawing of Sasuke. What do u think guys?  \n",
       "99  Now all picture is clear for me that Who can d...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_type = [post_type for post_type in\n",
    "#                      soup.find_all('div', {\"class\": \"top matter\" })]\n",
    "\n",
    "# for i in post_type:\n",
    "#     print(i.find_all('div').attrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run scraper every 2 minutes for a few hours\n",
    "# time posted\n",
    "# post type -video, external link, \n",
    "# get time stamp for each scrape \n",
    "# username of poster, \n",
    "# karma points of poster- post karma, comment karma\n",
    "# external link- what websites are posts going to \n",
    "# how many followers does the subreddit page have\n",
    "# is post relevant to subreddit?\n",
    "\n",
    "\n",
    "        \n",
    "       \n",
    "# post_type =\n",
    "        \n",
    "#         user_name = \n",
    "#         user_commment_karma = \n",
    "#         user_post_karma = \n",
    "#         external_link_site =\n",
    "#         num_subr_followers = \n",
    "#         time_posted= \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "df.to_csv('/Users/meredithjackson/Desktop/ga/week5/reddit_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting comments using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "243e949e-2742-40af-872e-fec475fd306c"
   },
   "source": [
    "#### Load in the the data of scraped results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - whether the number of comments was low or high. Compute the median number of comments and create a new binary variable that is true when the number of comments is high (above the median)\n",
    "\n",
    "We could also perform Linear Regression (or any regression) to predict the number of comments here. Instead, we are going to convert this into a _binary_ classification problem, by predicting two classes, HIGH vs LOW number of comments.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extremely popular threads. We don't _have_ to choose the `median` as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of comment numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a Random Forest model to predict High/Low number of comments using Sklearn. Start by ONLY using the subreddit as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "0ef04f32-419c-4bf2-baf7-48201f03df89"
   },
   "source": [
    "#### Create a few new variables in your dataframe to represent interesting features of a thread title.\n",
    "- For example, create a feature that represents whether 'cat' is in the title or whether 'funny' is in the title. \n",
    "- Then build a new Random Forest with these features. Do they add any value?\n",
    "- After creating these variables, use count-vectorizer to create features based on the words in the thread titles.\n",
    "- Build a new random forest model with subreddit and these new features included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process with a non-tree-based method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "#### Use Count Vectorizer from scikit-learn to create features from the thread titles. \n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "3be94357-e551-4094-b784-2df039216d33"
   },
   "source": [
    "### BONUS\n",
    "Refer to the README for the bonus parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "4239e458-28bd-4675-8db3-c1d9c02b9854"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
